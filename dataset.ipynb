{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\ME-COMPUTERS\\THIRD SEM\\ADM\\project\n"
     ]
    }
   ],
   "source": [
    "cd \"E:\\ME-COMPUTERS\\THIRD SEM\\ADM\\project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys, requests, shutil, os\n",
    "\n",
    "data=pd.read_csv(\"train.csv\")\n",
    "data_test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523603"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmark_list = [str(x) for x in list(range(1000,7000))]\n",
    "data_sample = data[data['landmark_id'].isin(landmark_list)]\n",
    "counts=data_sample.landmark_id.value_counts()\n",
    "sum(data_sample.landmark_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. URLs overwritten\n",
      "2. train and test set created\n",
      "3. train and validation set created\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "TARGET_SIZE = 96 #imports images of resolution 96x96\n",
    "\n",
    "'''change URLs to resize images to target size'''\n",
    "def overwrite_urls(df):\n",
    "    def reso_overwrite(url_tail, reso=TARGET_SIZE):\n",
    "        pattern = 's[0-9]+'\n",
    "        search_result = re.match(pattern, url_tail)\n",
    "        if search_result is None:\n",
    "            return url_tail\n",
    "        else:\n",
    "            return 's{}'.format(reso)\n",
    "    \n",
    "    def join_url(parsed_url, s_reso):\n",
    "        parsed_url[-2] = s_reso\n",
    "        return '/'.join(parsed_url)\n",
    "    \n",
    "    df = df[df.url.apply(lambda x: len(x.split('/'))>1)]\n",
    "    parsed_url = df.url.apply(lambda x: x.split('/'))\n",
    "    train_url_tail = parsed_url.apply(lambda x: x[-2])\n",
    "    resos = train_url_tail.apply(lambda x: reso_overwrite(x, reso=TARGET_SIZE))\n",
    "\n",
    "    overwritten_df = pd.concat([parsed_url, resos], axis=1)\n",
    "    overwritten_df.columns = ['url', 's_reso']\n",
    "    df['url'] = overwritten_df.apply(lambda x: join_url(x['url'], x['s_reso']), axis=1)\n",
    "    return df\n",
    "\n",
    "data_sample_resize = overwrite_urls(data_sample)\n",
    "print ('1. URLs overwritten')\n",
    "\n",
    "'''Split to test and train'''\n",
    "data_test = pd.DataFrame(columns = ['id','url','landmark_id'])\n",
    "data_training_all = pd.DataFrame(columns = ['id','url','landmark_id'])\n",
    "percent_test = 0.05\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "for landmark_id in set(data_sample_resize['landmark_id']):\n",
    "    n=1\n",
    "    t = data_sample_resize[(data_sample_resize.landmark_id == landmark_id)] #get all images for a landmark id\n",
    "    i = 0\n",
    "    r =[]\n",
    "    #print(len(t.id))\n",
    "    while i < len(t.id) and i<6000:\n",
    "        it = i\n",
    "        r.append(t.id.iloc[it])  #create a list of all these images\n",
    "        i += 1\n",
    "        \n",
    "    test = random.sample(r,int(percent_test*len(r))) #randomly pick a sample of 1% images from list 'r'\n",
    "    training = list(set(r) - set(test))  #get the remaining images\n",
    "    data_t = data_sample_resize[data_sample_resize.id.isin(test)] #holdout dataset\n",
    "    data_tr = data_sample_resize[data_sample_resize.id.isin(training)] #training dataset\n",
    "    data_test = data_test.append(data_t)  \n",
    "    data_training_all = data_training_all.append(data_tr)\n",
    "    n+=1\n",
    "\n",
    "print ('2. train and test set created')\n",
    "\n",
    "\n",
    "'''Split into train and validation set'''\n",
    "data_valid = pd.DataFrame(columns = ['id','url','landmark_id'])\n",
    "data_train = pd.DataFrame(columns = ['id','url','landmark_id'])\n",
    "percent_validation = 0.2 #takes 20% from each class as holdout data\n",
    "import random\n",
    "random.seed(42)\n",
    "for landmark_id in set(data_training_all['landmark_id']):\n",
    "    n=1\n",
    "    t = data_training_all[(data_training_all.landmark_id == landmark_id)]\n",
    "    i = 0\n",
    "    r =[]\n",
    "    while i < len(t.id):\n",
    "        it = i\n",
    "        r.append(t.id.iloc[it])\n",
    "        i += 1\n",
    "        \n",
    "    valid = random.sample(r,int(percent_validation*len(r)))\n",
    "    train = list(set(r) - set(valid)) \n",
    "    data_v = data_training_all[data_training_all.id.isin(valid)]\n",
    "    data_t = data_training_all[data_training_all.id.isin(train)]\n",
    "    data_valid = data_valid.append(data_v)\n",
    "    data_train = data_train.append(data_t)\n",
    "    n+=1\n",
    "\n",
    "print ('3. train and validation set created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample=data_train.to_csv()\n",
    "test_sample=data_test.to_csv()\n",
    "valdi_sample=data_test.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create directories 'train_images_model', 'validation_images_model', 'test_images_from_train' before running the code ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image(path,folder):\n",
    "    try:\n",
    "        url=path\n",
    "        response=requests.get(url, stream=True)\n",
    "        with open(folder + '/image.jpg', 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "        del response\n",
    "    except:\n",
    "        print(\"error\")   \n",
    "'''TRAIN SET - fetch images for the resized URLs and save in the already created directory train_images_model'''\n",
    "i=0\n",
    "for link in data_train['url']: \n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    #looping over links to get images\n",
    "    if os.path.exists('train_images_model/'+str(data_train['id'].iloc[i])+'.jpg'):\n",
    "        i+=1\n",
    "        print(i)\n",
    "        continue\n",
    "    fetch_image(link,'train_images_model')\n",
    "    try:\n",
    "        os.rename('train_images_model/image.jpg','train_images_model/'+ str(data_train['id'].iloc[i])+ '.jpg')\n",
    "    except:\n",
    "        print(\"not found\")\n",
    "    i+=1\n",
    "#     if(i==50):   #uncomment to test in your machine\n",
    "#         break\n",
    "print('4. train images fetched')\n",
    "i=0\n",
    "for link in data_valid['url']:              #looping over links to get images\n",
    "    if os.path.exists('validation_images_model/'+str(data_valid['id'].iloc[i])+'.jpg'):\n",
    "        i+=1\n",
    "        continue\n",
    "    fetch_image(link,'validation_images_model')\n",
    "    try:\n",
    "        os.rename('validation_images_model/image.jpg','validation_images_model/'+ str(data_valid['id'].iloc[i])+ '.jpg')\n",
    "    except:\n",
    "        print(\"not found\")\n",
    "    i+=1\n",
    "#     if(i==50):   #uncomment to test in your machine\n",
    "#         break\n",
    "print('5. Validation images fetched')\n",
    "\n",
    "i=0\n",
    "for link in data_test['url']:              #looping over links to get images\n",
    "    if os.path.exists('test_images_from_train/'+str(data_test['id'].iloc[i])+'.jpg'):\n",
    "        i+=1\n",
    "        continue\n",
    "    fetch_image(link,'test_images_from_train')\n",
    "    try:\n",
    "        os.rename('test_images_from_train/image.jpg','test_images_from_train/'+ str(data_test['id'].iloc[i])+ '.jpg')\n",
    "    except:\n",
    "        print(\"not found\")\n",
    "    i+=1\n",
    "#     if(i==50):   #uncomment to test in your machine\n",
    "#         break\n",
    "print('6. Test images fetched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Creating folders for each landmark ID (Class label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create folders for landmark IDs in Training folder\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import urllib\n",
    "\n",
    "train_data = data_train\n",
    "\n",
    "temp = pd.DataFrame(data_train.landmark_id.value_counts())\n",
    "temp.reset_index(inplace=True)\n",
    "temp.columns = ['landmark_id','count']\n",
    "\n",
    "def createfolders(dataset,folder):\n",
    "    i = 0\n",
    "    while i < len(dataset):\n",
    "        landmark = str(dataset.landmark_id.iloc[i])\n",
    "        path = folder + '/'+ landmark\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        i+=1\n",
    "createfolders(temp,'train_images_model')\n",
    "available = [int((x[0].split('/'))[-1]) for x in os.walk(r'train_images_model/') if len((x[0].split('/'))[-1]) > 0]\n",
    "new = [str(x) for x in range(1000,6999) if x not in available]\n",
    "for i in new:\n",
    "    path = 'train_images_model/' + i\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "print ('Train folders created')\n",
    "\n",
    "rootdirpics = r'train_images_model/'\n",
    "rootdirfolders = r'train_images_model/'\n",
    "\n",
    "def transformdata(data,path1, path2):\n",
    "\n",
    "    n = 1\n",
    "    for landmark_id in set(data['landmark_id']):\n",
    "        t = data[(data.landmark_id == landmark_id)]\n",
    "        i = 1\n",
    "        r =[]\n",
    "        while i <= len(t.id):\n",
    "            it = i - 1\n",
    "            r.append(t.id.iloc[it])\n",
    "            i += 1\n",
    "        for files in os.listdir(rootdirpics):    # loop through startfolders\n",
    "            inpath = path1 + files\n",
    "            folder = str(landmark_id)\n",
    "            outpath = path2 + folder  \n",
    "            if ((files.split('.')[0] in r) & (os.path.getsize(inpath) >1000)):\n",
    "#                 print('move')\n",
    "                shutil.move(inpath, outpath)\n",
    "            elif ((files.split('.')[0] in r) & (os.path.getsize(inpath) <= 1000)):\n",
    "                os.remove(inpath)\n",
    "        n+=1\n",
    "\n",
    "transformdata(train_data,rootdirpics, rootdirfolders)\n",
    "print ('Train images moved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create folders for landmark IDs in Validation folder\n",
    "\n",
    "temp = pd.DataFrame(data_valid.landmark_id.value_counts())\n",
    "temp.reset_index(inplace=True)\n",
    "temp.columns = ['landmark_id','count']\n",
    "createfolders(temp,'validation_images_model')\n",
    "print ('Validation folders created')\n",
    "\n",
    "#make folders for landmark ID which had no images in validation sets - required for codes running next\n",
    "available = [int((x[0].split('/'))[-1]) for x in os.walk(r'validation_images_model/') if len((x[0].split('/'))[-1]) > 0]\n",
    "new = [str(x) for x in range(1000,6999) if x not in available]\n",
    "for i in new:\n",
    "    path = 'validation_images_model/' + i\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "rootdirpics = r'validation_images_model/'\n",
    "rootdirfolders = r'validation_images_model/'\n",
    "transformdata(data_valid,rootdirpics, rootdirfolders)\n",
    "print ('Validation images moved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove corrupted images from the dataset\n",
    "def cleaning(dir):\n",
    "    i = 1000\n",
    "    count = 0\n",
    "    while i <= 6999:\n",
    "        f = str(i)\n",
    "        print (f)\n",
    "        for root, dirs, files in os.walk(dir +'/'+ f):  # loop through startfolders\n",
    "            for pic in files:\n",
    "                p=dir+'/'+f+'/'+pic  \n",
    "                try:\n",
    "                    im=Image.open(p)\n",
    "                except IOError:\n",
    "                    count+=1\n",
    "                    print(p)\n",
    "                    os.remove(p)\n",
    "            i += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning(\"/train_images_model\")\n",
    "cleaning(\"/validation_images_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
